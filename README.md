# WSLCB Licensing Tracker

A web application that scrapes, archives, and provides searchable access to licensing activity published by the **Washington State Liquor and Cannabis Board (WSLCB)**.

The Board publishes a [rolling 30-day report](https://licensinginfo.lcb.wa.gov/EntireStateWeb.asp) of new license applications, approvals, and discontinuances for alcohol, cannabis, tobacco, and vapor product businesses across Washington State. This tracker scrapes that report twice daily and preserves the data in a searchable database, building a historical archive that extends beyond the 30-day window.

A project of [Cannabis Observer](https://cannabis.observer/) ðŸŒ±ðŸ›ï¸ðŸ”

**Live instance:** [https://wslcb-licensing-tracker.exe.xyz:8000/](https://wslcb-licensing-tracker.exe.xyz:8000/)

## Features

- **Twice-daily automated scraping** of the WSLCB statewide licensing activity page
- **Full-text search** across business names, locations, applicants, license types, and license numbers
- **Normalized endorsements** â€” numeric license codes from approved/discontinued records are resolved to human-readable names via a managed codeâ†’endorsement mapping
- **Address standardization** â€” raw business addresses are parsed into structured components (street, suite, city, state, ZIP) via an external validation API, fixing ~6% of records with mis-parsed cities
- **Filterable results** by record type, application type, endorsement, state, city, and date range
- **Record detail pages** with related records for the same license number
- **CSV export** of any search result set
- **Historical archive** â€” the source only shows 30 days, but the database retains all data
- **License transfer tracking** â€” ASSUMPTION records capture both seller and buyer business names and applicants
- **Location change tracking** â€” CHANGE OF LOCATION records capture both previous and new business addresses
- **Entity normalization** â€” applicant names (people and organizations) are extracted into a shared `entities` table with name cleaning (uppercasing, stray punctuation removal), enabling cross-license analysis (e.g., "show all licenses for person X")
- **Deduplication** â€” safe to re-scrape; duplicate records are automatically skipped

## Data

The tracker captures three categories of licensing activity:

| Section | Description |
|---|---|
| **New Applications** | Businesses that have submitted new license applications, renewals, changes of location, tradename changes, corporate officer changes, and other application types |
| **Approved** | Licenses recently approved by the Board |
| **Discontinued** | Licenses that have been discontinued |

Each record includes:

| Field | Description |
|---|---|
| Date | Notification, approval, or discontinuance date |
| Business Name | Registered business name (for ASSUMPTION records, the buyer's business name) |
| Business Location | Full street address including city, state, and ZIP (raw and standardized components) |
| Applicant(s) | Named applicants (new applications only; for ASSUMPTION records, the buyer's applicants) |
| Previous Business Name | Seller's business name (ASSUMPTION records only) |
| Previous Applicant(s) | Seller's applicants (ASSUMPTION records only) |
| Previous Location | Origin address before relocation (CHANGE OF LOCATION records only, new applications section) |
| Endorsements | One or more license/endorsement types (e.g., "CANNABIS RETAILER", "GROCERY STORE - BEER/WINE"), normalized from text names or numeric WSLCB codes |
| Application Type | RENEWAL, NEW APPLICATION, CHANGE OF LOCATION, ASSUMPTION, DISCONTINUED, etc. |
| License Number | WSLCB license number |
| Contact Phone | Business contact phone number |

## Architecture

| Component | Technology |
|---|---|
| Scraper | Python, [httpx](https://www.python-httpx.org/), [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) + lxml |
| Address validation | External API ([address-validator](https://address-validator.exe.xyz:8000/docs)) for USPS-standardized parsing |
| Database | SQLite with [FTS5](https://www.sqlite.org/fts5.html) full-text search |
| Web framework | [FastAPI](https://fastapi.tiangolo.com/) with [Jinja2](https://jinja.palletsprojects.com/) templates |
| Frontend | Server-rendered HTML, [HTMX](https://htmx.org/), [Tailwind CSS](https://tailwindcss.com/) (CDN, custom brand palette) |
| Scheduling | systemd timer (twice-daily) |

## Logging

All modules use Pythonâ€™s `logging` module via a centralized configuration in `log_config.py`. Output format is auto-detected:

- **Interactive terminal (TTY):** human-readable with timestamps â€” `2026-02-23 04:18:47  INFO      scraper  Starting scrape of ...`
- **systemd / pipe (non-TTY):** JSON lines via [python-json-logger](https://github.com/madzak/python-json-logger) â€” `{"timestamp": "2026-02-23T04:18:47", "level": "INFO", "name": "scraper", "message": "Starting scrape of ..."}`

Uvicornâ€™s access and error logs are routed through the same formatter for consistent output.

## Project Structure

```
wslcb-licensing-tracker/
â”œâ”€â”€ app.py                  # FastAPI web application
â”œâ”€â”€ cli.py                  # Unified CLI entry point (argparse subcommands)
â”œâ”€â”€ pipeline.py             # Unified ingestion pipeline (ingest_record, ingest_batch)
â”œâ”€â”€ display.py              # Presentation formatting (format_outcome, summarize_provenance)
â”œâ”€â”€ parser.py               # Pure HTML/diff parsing (no DB, no side effects)
â”œâ”€â”€ db.py                   # Connection management, constants (thin base layer)
â”œâ”€â”€ schema.py               # DDL, PRAGMA user_version migrations, FTS5, seeding
â”œâ”€â”€ database.py             # Backward-compat shim + location/source/provenance helpers
â”œâ”€â”€ queries.py              # Record search, filters, stats, CRUD
â”œâ”€â”€ entities.py             # Entity (applicant) normalization
â”œâ”€â”€ migrate_locations.py    # Legacy migration (absorbed into schema.py; retained for reference)
â”œâ”€â”€ endorsements.py         # License endorsement normalization (codeâ†”name mappings)
â”œâ”€â”€ log_config.py           # Centralized logging configuration
â”œâ”€â”€ address_validator.py    # Address validation API client
â”œâ”€â”€ scraper.py              # WSLCB page scraper â€” fetch, archive, ingest via pipeline
â”œâ”€â”€ backfill_snapshots.py   # Ingest + repair from archived HTML snapshots via pipeline
â”œâ”€â”€ backfill_diffs.py       # Ingest from CO diff archives via pipeline
â”œâ”€â”€ backfill_provenance.py  # One-time backfill of source provenance links
â”œâ”€â”€ env                     # API keys (gitignored, 640 root:exedev)
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.html           # Base layout template
â”‚   â”œâ”€â”€ index.html          # Dashboard with stats
â”‚   â”œâ”€â”€ search.html         # Search interface with filters
â”‚   â”œâ”€â”€ detail.html         # Record detail page
â”‚   â”œâ”€â”€ 404.html            # Not-found error page
â”‚   â”œâ”€â”€ entity.html         # Entity detail page
â”‚   â””â”€â”€ partials/
â”‚       â”œâ”€â”€ results.html      # Search results partial (HTMX)
â”‚       â””â”€â”€ record_table.html # Shared record table (results + entity pages)
â”œâ”€â”€ static/                 # Static assets
â”‚   â””â”€â”€ images/             # Cannabis Observer brand assets (icon + wordmark SVGs)
â”œâ”€â”€ data/                   # Persistent data (gitignored)
â”‚   â”œâ”€â”€ wslcb.db            # SQLite database
â”‚   â””â”€â”€ wslcb/                  # Archived data by source
â”‚       â”œâ”€â”€ licensinginfo/      # HTML snapshots from licensinginfo.lcb.wa.gov
â”‚       â”‚   â””â”€â”€ [yyyy]/         # Archived HTML snapshots by year
â”‚       â””â”€â”€ licensinginfo-diffs/  # CO diff archive files
â”‚           â”œâ”€â”€ notifications/  # Unified diffs of the notifications section
â”‚           â”œâ”€â”€ approvals/      # Unified diffs of the approvals section
â”‚           â””â”€â”€ discontinued/   # Unified diffs of the discontinued section
â”œâ”€â”€ requirements.txt        # Python dependencies (runtime + dev)
â”œâ”€â”€ pytest.ini              # Pytest configuration
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ conftest.py         # Shared fixtures (in-memory DB, sample records)
â”‚   â”œâ”€â”€ test_parser.py      # Parser function tests
â”‚   â”œâ”€â”€ test_db.py          # Connection management and constant tests
â”‚   â”œâ”€â”€ test_schema.py      # Migration framework tests
â”‚   â”œâ”€â”€ test_database.py    # Database helper tests (location/source/provenance)
â”‚   â”œâ”€â”€ test_queries.py     # Record insert/query tests
â”‚   â””â”€â”€ fixtures/           # Minimal HTML fixtures for parser tests
â”œâ”€â”€ wslcb-web.service       # systemd service for the web app
â”œâ”€â”€ wslcb-task@.service     # systemd template for oneshot tasks (scrape, refresh, backfill)
â””â”€â”€ wslcb-scraper.timer     # systemd timer (twice-daily: 12:30 AM and 6:30 AM Pacific)
```

## Setup

### Prerequisites

- Python 3.12+
- systemd (for scheduling; optional if running manually)

### Installation

```bash
git clone https://github.com/CannObserv/wslcb-licensing-tracker.git
cd wslcb-licensing-tracker

python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Run the initial scrape

```bash
python cli.py scrape
```

This fetches the current 30-day report, populates the SQLite database (`data/wslcb.db`), and archives a copy of the source HTML under `data/wslcb/licensinginfo/`.

### Start the web application

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

Then visit [http://localhost:8000](http://localhost:8000).

### Set up automated scraping (systemd)

```bash
sudo cp wslcb-web.service /etc/systemd/system/
sudo cp wslcb-task@.service /etc/systemd/system/
sudo cp wslcb-scraper.timer /etc/systemd/system/

sudo systemctl daemon-reload
sudo systemctl enable --now wslcb-web.service
sudo systemctl enable --now wslcb-scraper.timer
```

The scraper runs twice daily at 12:30 AM and 6:30 AM Pacific with up to 5 minutes of random delay.

Check the timer status:

```bash
systemctl list-timers wslcb-scraper.timer
journalctl -u wslcb-task@scrape.service   # scraper logs
journalctl -u wslcb-web.service       # web app logs
```

### Configuration

| Environment Variable | Default | Description |
|---|---|---|
| `ADDRESS_VALIDATOR_API_KEY` | *(none)* | API key for the address validation service (also read from `./env` file) |

The SQLite database and archived HTML snapshots are stored in `./data/` relative to the project root.

## API Endpoints

| Endpoint | Description |
|---|---|
| `GET /` | Dashboard with summary statistics |
| `GET /search` | Search interface (HTML) or HTMX partial results |
| `GET /record/{id}` | Record detail page |
| `GET /export` | CSV export (accepts same query params as `/search`) |
| `GET /entity/{id}` | Entity detail page â€” lists all license records associated with a person or organization, with type badge (Person/Organization), record count, and distinct license count |
| `GET /api/cities` | JSON list of cities for a given state (query param `state`) |
| `GET /api/stats` | JSON summary statistics |

## License Type Normalization

The WSLCB source page uses two different representations for license types:

- **New applications** list endorsements as semicolon-separated text (e.g., `GROCERY STORE - BEER/WINE; SNACK BAR`)
- **Approved/discontinued** records use opaque numeric codes (e.g., `450,`)

Historical data also uses a hybrid `"CODE, NAME"` format (e.g., `"450, GROCERY STORE - BEER/WINE"`). The tracker normalizes all three into a shared `license_endorsements` table, linked to records via a `record_endorsements` junction table. A seed mapping of 98 known codes is built into `endorsements.py`, and new mappings are automatically discovered by cross-referencing license numbers that appear in both sections.

## Address Standardization

Business addresses are stored in a normalized `locations` table â€” each unique raw address string from the WSLCB source is stored once and shared across all license records that reference it. Records link to locations via `location_id` and `previous_location_id` foreign keys.

Each location is standardized via an external address validation API into structured USPS-standard components:

| Field | Example |
|---|---|
| `address_line_1` | `1200 WESTLAKE AVE N` |
| `address_line_2` | `STE 100` |
| `std_city` | `SEATTLE` |
| `std_state` | `WA` |
| `std_zip` | `98109-3528` |

Locations are validated at scrape time. When a new record references an already-known address, it reuses the existing location row and skips the API call. Un-validated locations can be backfilled:

```bash
python cli.py backfill-addresses
```

To re-validate all locations (e.g., after the validation service is updated):

```bash
python cli.py refresh-addresses
```

This is safe to interrupt â€” progress is committed in batches and each location's timestamp is updated individually.

The original raw address string is always preserved in `locations.raw_address`. If the validation service is unavailable, the scrape completes normally and standardized fields remain empty until a future backfill.

## ASSUMPTION Records

ASSUMPTION records represent one business assuming (purchasing) a license from another. They contain data about both the seller and buyer, using different field labels than standard records:

| Source Field | DB Column | Description |
|---|---|---|
| Current Business Name | `previous_business_name` | Seller's business name |
| Current Applicant(s) | `previous_applicants` | Seller's applicants (often empty) |
| New Business Name | `business_name` | Buyer's business name |
| New Applicant(s) | `applicants` | Buyer's applicants |
| Business Location | `location_id` â†’ `locations` | Single shared location |
| Contact Phone | `contact_phone` | Buyer's contact phone |

## CHANGE OF LOCATION Records

CHANGE OF LOCATION records represent a business moving to a new physical address. In the new applications section, they use different field labels to capture both the origin and destination:

| Source Field | DB Column | Description |
|---|---|---|
| Current Business Location | `previous_location_id` â†’ `locations` | Origin address (moving from) |
| New Business Location | `location_id` â†’ `locations` | Destination address (moving to) |

Both the previous and new addresses are stored as entries in the `locations` table, each with their own regex-parsed and USPS-standardized components.

In the approved section, CHANGE OF LOCATION records only have `location_id` (the new address) â€” the source page does not provide the previous address for approved records.

## Entity Normalization

Applicant names (people and organizations behind each license) are extracted from the semicolon-delimited `applicants` and `previous_applicants` fields into a shared `entities` table. This enables cross-license analysis â€” e.g., viewing all licenses associated with a particular person.

Names are normalized at ingestion time:

- **Uppercased** for consistency (the WSLCB source is predominantly uppercase but occasionally uses mixed case)
- **Stray trailing punctuation stripped** â€” the source occasionally appends errant periods or commas to names (e.g., `WOLDU ARAYA BERAKI.`). These are removed while preserving legitimate suffixes like `INC.`, `JR.`, `SR.`, `LTD.`, `CORP.`, etc.
- **Deduplicated** by exact name match â€” the same person appearing on multiple licenses shares a single entity row

The first element of the `applicants` string (the business name) is excluded from entity extraction â€” only the individual people and organizations are stored.

The same cleaning is applied to the `applicants` and `previous_applicants` string columns on `license_records`, so FTS search results and CSV exports are consistent with entity names.

Entities are classified as `person` or `organization` by a heuristic that checks for business-indicator patterns (`LLC`, `INC`, `CORP`, `TRUST`, etc.).

## Source Provenance

Every license record is linked to the source artifact(s) it was extracted from via a many-to-many `record_sources` junction table. This tracks *where* each record came from (live scrape, archived snapshot, Internet Archive, etc.), when the source was captured, and what role the source played (first introduction, corroboration, or data repair).

### Source Types

| ID | Slug | Description |
|---|---|---|
| 1 | `live_scrape` | Direct scrape of the WSLCB licensing page |
| 2 | `co_archive` | Cannabis Observer archived HTML snapshots |
| 3 | `internet_archive` | Wayback Machine snapshots |
| 4 | `co_diff_archive` | Cannabis Observer diff-detected change snapshots |
| 5 | `manual` | Manually entered or corrected records |

### Roles

| Role | Meaning |
|---|---|
| `first_seen` | This source introduced the record to the database |
| `confirmed` | Record already existed; this source corroborates it |
| `repaired` | This source was used to fix/enrich the record |

Provenance is displayed on record detail pages as collapsed summary badges (e.g., "âš¡ Live Scrape Ã—13, ðŸ“ CO Archive Ã—10") with an expandable list of individual sources.

## Backfilling from Snapshots

To ingest historical records and repair broken data from archived HTML snapshots:

```bash
python cli.py backfill-snapshots
```

This runs a two-phase process:
1. **Ingest** â€” insert new records from all archived snapshots (duplicates are safely skipped)
2. **Repair** â€” fix broken ASSUMPTION records (empty business names) and CHANGE OF LOCATION records (missing locations)

Safe to re-run at any time. Address validation is deferred; run `python cli.py backfill-addresses` afterward to validate new locations.

## Testing

The project uses pytest with in-memory SQLite databases and static HTML fixtures. Tests run in under 1 second with no network calls.

```bash
source venv/bin/activate
python -m pytest tests/ -v
```

The project follows **red/green TDD**: every new feature and bug fix starts with a failing test, then the minimum code to make it pass, then refactoring.

Test structure:

| File | Scope |
|---|---|
| `tests/test_parser.py` | Pure HTML parsing functions â€” all record types, edge cases |
| `tests/test_db.py` | Connection management, constants, raw address normalization |
| `tests/test_schema.py` | Migration framework, user_version, existing DB detection |
| `tests/test_database.py` | Location/source/provenance helper functions |
| `tests/test_pipeline.py` | Unified ingestion pipeline â€” insert, endorsements, provenance, outcome linking |
| `tests/test_display.py` | Presentation formatting â€” outcome statuses, provenance summaries |
| `tests/test_queries.py` | Record insertion, deduplication, entity creation |
| `tests/conftest.py` | Shared fixtures: in-memory DB, sample record dicts |
| `tests/fixtures/` | Minimal HTML files exercising each record type and section |

## Data Source

All data is sourced from the Washington State Liquor and Cannabis Board's public licensing activity page:

**[https://licensinginfo.lcb.wa.gov/EntireStateWeb.asp](https://licensinginfo.lcb.wa.gov/EntireStateWeb.asp)**

> *STATEWIDE New License Applications, Approvals, and Discontinuances (for past 30 days, in order of most recent date)*

This is an unofficial tracker and is not affiliated with or endorsed by the WSLCB.

## License

MIT
